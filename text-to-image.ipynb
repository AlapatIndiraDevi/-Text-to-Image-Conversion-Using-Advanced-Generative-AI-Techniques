{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZsZ16jwRLdX",
        "outputId": "beddce2d-b3f1-425c-c044-0e77184b7536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q diffusers transformers accelerate streamlit\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4K6ukzNQYgO",
        "outputId": "57153e07-f3d9-472c-a965-4f2900e5d9c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "import time\n",
        "import random\n",
        "import io\n",
        "import base64\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"AI Dream Canvas | Stable Diffusion Generator\",\n",
        "    page_icon=\"🎨\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Apply custom CSS\n",
        "def load_css():\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "        .main .block-container {\n",
        "            padding-top: 2rem;\n",
        "            padding-bottom: 2rem;\n",
        "        }\n",
        "        h1, h2, h3 {\n",
        "            color: #1E88E5;\n",
        "        }\n",
        "        .stButton > button {\n",
        "            background-color: #1E88E5;\n",
        "            color: white;\n",
        "            font-weight: bold;\n",
        "            border-radius: 8px;\n",
        "            padding: 0.5rem 1rem;\n",
        "            transition: all 0.3s;\n",
        "        }\n",
        "        .stButton > button:hover {\n",
        "            background-color: #1565C0;\n",
        "            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
        "        }\n",
        "        .image-card {\n",
        "            border: 1px solid #e0e0e0;\n",
        "            border-radius: 10px;\n",
        "            padding: 1rem;\n",
        "            background-color: white;\n",
        "            box-shadow: 0 4px 6px rgba(0,0,0,0.05);\n",
        "        }\n",
        "        .sidebar .block-container {\n",
        "            padding-top: 2rem;\n",
        "        }\n",
        "        .stSlider > div > div {\n",
        "            color: #1E88E5 !important;\n",
        "        }\n",
        "        .gallery-image {\n",
        "            border-radius: 8px;\n",
        "            transition: transform 0.3s;\n",
        "        }\n",
        "        .gallery-image:hover {\n",
        "            transform: scale(1.02);\n",
        "        }\n",
        "        .status-indicator {\n",
        "            padding: 0.3rem 0.6rem;\n",
        "            border-radius: 20px;\n",
        "            font-size: 0.8rem;\n",
        "            font-weight: bold;\n",
        "        }\n",
        "        .status-ready {\n",
        "            background-color: #4CAF50;\n",
        "            color: white;\n",
        "        }\n",
        "        .status-busy {\n",
        "            background-color: #FFC107;\n",
        "            color: black;\n",
        "        }\n",
        "        footer {visibility: hidden;}\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "load_css()\n",
        "\n",
        "# Initialize session state variables\n",
        "if 'history' not in st.session_state:\n",
        "    st.session_state.history = []\n",
        "if 'current_image' not in st.session_state:\n",
        "    st.session_state.current_image = None\n",
        "if 'current_prompt' not in st.session_state:\n",
        "    st.session_state.current_prompt = \"\"\n",
        "if 'model_loaded' not in st.session_state:\n",
        "    st.session_state.model_loaded = False\n",
        "if 'pipe' not in st.session_state:\n",
        "    st.session_state.pipe = None\n",
        "if 'gallery_view' not in st.session_state:\n",
        "    st.session_state.gallery_view = False\n",
        "if 'saved_images' not in st.session_state:\n",
        "    # Create directory to save images if it doesn't exist\n",
        "    if not os.path.exists(\"generated_images\"):\n",
        "        os.makedirs(\"generated_images\")\n",
        "    st.session_state.saved_images = []\n",
        "\n",
        "# Function to get image download link\n",
        "def get_image_download_link(img, filename, text):\n",
        "    buffered = io.BytesIO()\n",
        "    img.save(buffered, format=\"PNG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "    href = f'<a href=\"data:image/png;base64,{img_str}\" download=\"{filename}\" style=\"display: inline-block; padding: 0.5rem 1rem; background-color: #4CAF50; color: white; text-decoration: none; border-radius: 4px; text-align: center; font-weight: bold;\">{text}</a>'\n",
        "    return href\n",
        "\n",
        "# Function to save image to disk\n",
        "def save_image(image, prompt):\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"generated_images/sd_{timestamp}.png\"\n",
        "    image.save(filename)\n",
        "    return {\n",
        "        \"filename\": filename,\n",
        "        \"prompt\": prompt,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"image\": image\n",
        "    }\n",
        "\n",
        "# Function to load model - adapted for Colab\n",
        "@st.cache_resource\n",
        "def load_model(model_id, scheduler_type, torch_dtype):\n",
        "    if scheduler_type == \"DPM++ 2M\":\n",
        "        scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
        "            model_id, subfolder=\"scheduler\", solver_order=2\n",
        "        )\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            scheduler=scheduler,\n",
        "            torch_dtype=torch_dtype\n",
        "        )\n",
        "    else:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch_dtype\n",
        "        )\n",
        "\n",
        "    # Move to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        pipe = pipe.to(\"cuda\")\n",
        "\n",
        "        # Enable attention slicing for lower memory usage\n",
        "        pipe.enable_attention_slicing()\n",
        "\n",
        "        # Safely check if we can use memory efficient attention\n",
        "        # Skip xformers since it can cause issues in Colab\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Generate random prompts for inspiration\n",
        "def get_random_prompt():\n",
        "    styles = [\"photorealistic\", \"oil painting\", \"watercolor\", \"digital art\", \"pencil sketch\", \"3D render\", \"anime style\"]\n",
        "    subjects = [\"mountain landscape\", \"futuristic cityscape\", \"magical forest\", \"ocean sunset\", \"cosmic nebula\", \"fantasy character\", \"cyberpunk street\"]\n",
        "    adjectives = [\"vibrant\", \"mysterious\", \"ethereal\", \"dramatic\", \"peaceful\", \"dystopian\", \"surreal\"]\n",
        "    details = [\"detailed\", \"high quality\", \"intricate\", \"professional\", \"8K\", \"trending on artstation\", \"award-winning\"]\n",
        "\n",
        "    style = random.choice(styles)\n",
        "    subject = random.choice(subjects)\n",
        "    adjective = random.choice(adjectives)\n",
        "    detail = random.choice(details)\n",
        "\n",
        "    return f\"A {adjective} {subject}, {style}, {detail}\"\n",
        "\n",
        "# Function to generate image\n",
        "def generate_image(prompt, negative_prompt, num_inference_steps, guidance_scale, width, height, seed, model_id, scheduler_type):\n",
        "    try:\n",
        "        # Select device and data type\n",
        "        torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        torch_dtype = torch.float16 if torch_device == \"cuda\" else torch.float32\n",
        "\n",
        "        # Load or get model\n",
        "        if not st.session_state.model_loaded or st.session_state.current_model_id != model_id or st.session_state.current_scheduler != scheduler_type:\n",
        "            with st.spinner(\"Loading model... This may take a minute.\"):\n",
        "                st.session_state.pipe = load_model(model_id, scheduler_type, torch_dtype)\n",
        "                st.session_state.model_loaded = True\n",
        "                st.session_state.current_model_id = model_id\n",
        "                st.session_state.current_scheduler = scheduler_type\n",
        "\n",
        "        pipe = st.session_state.pipe\n",
        "\n",
        "        # Set up generator for reproducibility\n",
        "        generator = None\n",
        "        if seed != 0:\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(seed)\n",
        "\n",
        "        # Generate the image\n",
        "        start_time = time.time()\n",
        "        output = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            generator=generator\n",
        "        )\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Get the image\n",
        "        image = output.images[0]\n",
        "\n",
        "        # Calculate generation time\n",
        "        generation_time = end_time - start_time\n",
        "\n",
        "        # Add to history\n",
        "        image_info = {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"inference_steps\": num_inference_steps,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"dimensions\": f\"{width}x{height}\",\n",
        "            \"seed\": seed if seed != 0 else \"random\",\n",
        "            \"model\": model_id.split(\"/\")[-1],\n",
        "            \"generation_time\": f\"{generation_time:.2f}s\",\n",
        "            \"image\": image,\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "        st.session_state.history.append(image_info)\n",
        "        st.session_state.current_image = image\n",
        "        st.session_state.current_prompt = prompt\n",
        "\n",
        "        return image, image_info\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating image: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# App header\n",
        "def app_header():\n",
        "    col1, col2 = st.columns([6, 1])\n",
        "    with col1:\n",
        "        st.markdown(\"# 🎨 AI Dream Canvas\")\n",
        "        st.markdown(\"#### Transform your imagination into stunning visuals with Stable Diffusion\")\n",
        "\n",
        "    with col2:\n",
        "        device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
        "        status_class = \"status-ready\" if device == \"GPU\" else \"status-busy\"\n",
        "        st.markdown(f\"\"\"\n",
        "        <div style=\"text-align: right; margin-top: 1rem;\">\n",
        "            <span class=\"status-indicator {status_class}\">\n",
        "                {device}\n",
        "            </span>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Main app\n",
        "def main():\n",
        "    app_header()\n",
        "\n",
        "    # Sidebar - Model Settings\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"## 🔧 Model Settings\")\n",
        "\n",
        "        # Model selection\n",
        "        model_options = {\n",
        "            \"CompVis/stable-diffusion-v1-4\": \"Stable Diffusion v1.4\",\n",
        "            \"runwayml/stable-diffusion-v1-5\": \"Stable Diffusion v1.5\"\n",
        "        }\n",
        "        model_id = st.selectbox(\n",
        "            \"Model\",\n",
        "            options=list(model_options.keys()),\n",
        "            format_func=lambda x: model_options[x],\n",
        "            index=1  # Default to v1.5\n",
        "        )\n",
        "\n",
        "        # Scheduler selection\n",
        "        scheduler_type = st.selectbox(\n",
        "            \"Scheduler\",\n",
        "            options=[\"Default\", \"DPM++ 2M\"],\n",
        "            index=1  # Default to DPM++ 2M\n",
        "        )\n",
        "\n",
        "        # Resolution settings\n",
        "        st.markdown(\"### 📐 Resolution\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            width = st.select_slider(\n",
        "                \"Width\",\n",
        "                options=[384, 512, 576, 640],\n",
        "                value=512\n",
        "            )\n",
        "        with col2:\n",
        "            height = st.select_slider(\n",
        "                \"Height\",\n",
        "                options=[384, 512, 576, 640],\n",
        "                value=512\n",
        "            )\n",
        "\n",
        "        st.markdown(\"### ⚙️ Generation Parameters\")\n",
        "\n",
        "        # Quality vs. Speed slider\n",
        "        quality_speed = st.slider(\n",
        "            \"Quality vs. Speed\",\n",
        "            min_value=1,\n",
        "            max_value=5,\n",
        "            value=3,\n",
        "            help=\"1: Fastest (low quality), 5: Highest quality (slow)\"\n",
        "        )\n",
        "\n",
        "        # Map quality_speed to inference steps\n",
        "        quality_to_steps = {1: 20, 2: 25, 3: 30, 4: 40, 5: 50}\n",
        "        num_inference_steps = quality_to_steps[quality_speed]\n",
        "\n",
        "        # Advanced settings in expander\n",
        "        with st.expander(\"Advanced Settings\"):\n",
        "            # Allow direct adjustment of steps\n",
        "            num_inference_steps = st.slider(\n",
        "                \"Inference Steps\",\n",
        "                min_value=10,\n",
        "                max_value=75,\n",
        "                value=num_inference_steps,\n",
        "                help=\"Higher values = better quality but slower generation\"\n",
        "            )\n",
        "\n",
        "            guidance_scale = st.slider(\n",
        "                \"Guidance Scale\",\n",
        "                min_value=1.0,\n",
        "                max_value=20.0,\n",
        "                value=7.5,\n",
        "                step=0.5,\n",
        "                help=\"How strongly the image follows the prompt (higher = more faithful)\"\n",
        "            )\n",
        "\n",
        "            seed = st.number_input(\n",
        "                \"Random Seed\",\n",
        "                value=0,\n",
        "                help=\"0 for random, any other number for reproducible results\"\n",
        "            )\n",
        "\n",
        "        # View Toggle\n",
        "        st.markdown(\"### 🖼️ View Options\")\n",
        "        view_toggle = st.radio(\n",
        "            \"Display Mode\",\n",
        "            options=[\"Generator\", \"Gallery\"],\n",
        "            horizontal=True,\n",
        "            index=0 if not st.session_state.gallery_view else 1\n",
        "        )\n",
        "        st.session_state.gallery_view = (view_toggle == \"Gallery\")\n",
        "\n",
        "        # System info\n",
        "        device = \"CUDA (GPU)\" if torch.cuda.is_available() else \"CPU\"\n",
        "        memory_info = \"\"\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "                memory_info = f\", {gpu_memory:.1f} GB VRAM\"\n",
        "            except:\n",
        "                memory_info = \"\"\n",
        "\n",
        "        st.info(f\"Running on: {device}{memory_info}\")\n",
        "\n",
        "        # Colab specific tips\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"#### 💡 Colab Tips\")\n",
        "        st.markdown(\n",
        "            \"- First generation may take longer while model downloads\\n\"\n",
        "            \"- For stable performance, use Colab with GPU runtime\\n\"\n",
        "            \"- Larger resolutions require more VRAM\"\n",
        "        )\n",
        "\n",
        "    # Main content area\n",
        "    if not st.session_state.gallery_view:\n",
        "        # Generator View\n",
        "        col1, col2 = st.columns([5, 6])\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(\"### 🖋 Enter your prompt\")\n",
        "\n",
        "            # Prompt input area with examples\n",
        "            prompt = st.text_area(\n",
        "                \"Describe the image you want to generate\",\n",
        "                st.session_state.current_prompt or \"A futuristic cityscape at sunset with flying cars, detailed digital art\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            # Random prompt button\n",
        "            col1a, col1b = st.columns([1, 1])\n",
        "            with col1a:\n",
        "                if st.button(\"🎲 Random Prompt\", use_container_width=True):\n",
        "                    prompt = get_random_prompt()\n",
        "                    st.session_state.current_prompt = prompt\n",
        "                    st.rerun()\n",
        "\n",
        "            # Example prompts\n",
        "            with st.expander(\"✨ Prompt Examples\"):\n",
        "                example_prompts = [\n",
        "                    \"A serene Japanese garden with cherry blossoms, 4K, highly detailed\",\n",
        "                    \"Cyberpunk portrait of a young woman with neon lights, digital art\",\n",
        "                    \"Mystical forest with glowing mushrooms and fairy lights, fantasy concept art\",\n",
        "                    \"An astronaut riding a horse on Mars, photorealistic style\"\n",
        "                ]\n",
        "\n",
        "                for i, ex_prompt in enumerate(example_prompts):\n",
        "                    if st.button(f\"Example {i+1}\", key=f\"example_{i}\", use_container_width=True):\n",
        "                        prompt = ex_prompt\n",
        "                        st.session_state.current_prompt = prompt\n",
        "                        st.rerun()\n",
        "\n",
        "            # Negative prompt\n",
        "            negative_prompt = st.text_area(\n",
        "                \"Negative prompt (things to avoid)\",\n",
        "                \"blurry, bad anatomy, bad hands, cropped, worst quality, low quality\",\n",
        "                height=80\n",
        "            )\n",
        "\n",
        "            # Generation button\n",
        "            generate_button = st.button(\n",
        "                \"🚀 Generate Image\",\n",
        "                type=\"primary\",\n",
        "                use_container_width=True\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            # Display current image or placeholder\n",
        "            st.markdown(\"### 🖼️ Generated Image\")\n",
        "\n",
        "            image_container = st.container()\n",
        "            with image_container:\n",
        "                if st.session_state.current_image is not None:\n",
        "                    st.image(\n",
        "                        st.session_state.current_image,\n",
        "                        caption=st.session_state.current_prompt,\n",
        "                        use_column_width=True\n",
        "                    )\n",
        "\n",
        "                    # Image info and download buttons\n",
        "                    col_save, col_download = st.columns(2)\n",
        "                    with col_save:\n",
        "                        if st.button(\"💾 Save to Gallery\", use_container_width=True):\n",
        "                            saved_info = save_image(st.session_state.current_image, st.session_state.current_prompt)\n",
        "                            st.session_state.saved_images.append(saved_info)\n",
        "                            st.success(\"Image saved to gallery!\")\n",
        "\n",
        "                    with col_download:\n",
        "                        download_link = get_image_download_link(\n",
        "                            st.session_state.current_image,\n",
        "                            \"stable_diffusion_image.png\",\n",
        "                            \"📥 Download Image\"\n",
        "                        )\n",
        "                        st.markdown(download_link, unsafe_allow_html=True)\n",
        "\n",
        "                    # Show generation info if available\n",
        "                    if st.session_state.history:\n",
        "                        latest = st.session_state.history[-1]\n",
        "                        with st.expander(\"Generation Details\", expanded=False):\n",
        "                            info_col1, info_col2 = st.columns(2)\n",
        "                            with info_col1:\n",
        "                                st.markdown(f\"**Model:** {latest['model']}\")\n",
        "                                st.markdown(f\"**Dimensions:** {latest['dimensions']}\")\n",
        "                                st.markdown(f\"**Seed:** {latest['seed']}\")\n",
        "                            with info_col2:\n",
        "                                st.markdown(f\"**Steps:** {latest['inference_steps']}\")\n",
        "                                st.markdown(f\"**Guidance Scale:** {latest['guidance_scale']}\")\n",
        "                                st.markdown(f\"**Generation Time:** {latest['generation_time']}\")\n",
        "                else:\n",
        "                    # Placeholder for when no image is generated yet\n",
        "                    st.markdown(\n",
        "                        \"\"\"\n",
        "                        <div style=\"\n",
        "                            border: 2px dashed #BDBDBD;\n",
        "                            border-radius: 10px;\n",
        "                            padding: 7rem 1rem;\n",
        "                            text-align: center;\n",
        "                            color: #9E9E9E;\n",
        "                        \">\n",
        "                            <h3>Your creation will appear here</h3>\n",
        "                            <p>Enter a prompt and click \"Generate Image\" to begin</p>\n",
        "                        </div>\n",
        "                        \"\"\",\n",
        "                        unsafe_allow_html=True\n",
        "                    )\n",
        "\n",
        "        # Generation history\n",
        "        if st.session_state.history:\n",
        "            st.markdown(\"---\")\n",
        "            st.markdown(\"### 📜 Recent Generations\")\n",
        "\n",
        "            # Display the last 5 generations in a horizontal scroll\n",
        "            history_items = list(reversed(st.session_state.history))[:5]\n",
        "\n",
        "            cols = st.columns(min(len(history_items), 5))\n",
        "            for i, history_item in enumerate(history_items[:5]):\n",
        "                with cols[i]:\n",
        "                    st.image(\n",
        "                        history_item[\"image\"],\n",
        "                        use_column_width=True,\n",
        "                        caption=history_item[\"prompt\"][:40] + \"...\" if len(history_item[\"prompt\"]) > 40 else history_item[\"prompt\"],\n",
        "                        output_format=\"PNG\",\n",
        "                        clamp=True\n",
        "                    )\n",
        "\n",
        "        # Handle image generation when button is clicked\n",
        "        if generate_button:\n",
        "            with st.spinner(\"🔮 Bringing your imagination to life...\"):\n",
        "                image, _ = generate_image(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    num_inference_steps=num_inference_steps,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    width=width,\n",
        "                    height=height,\n",
        "                    seed=seed,\n",
        "                    model_id=model_id,\n",
        "                    scheduler_type=scheduler_type\n",
        "                )\n",
        "                if image:\n",
        "                    st.rerun()\n",
        "\n",
        "    else:\n",
        "        # Gallery View\n",
        "        st.markdown(\"### 🖼️ Image Gallery\")\n",
        "\n",
        "        if not st.session_state.saved_images:\n",
        "            st.info(\"Your gallery is empty. Generate and save images to see them here.\")\n",
        "        else:\n",
        "            # Display saved images in a grid\n",
        "            num_cols = 3\n",
        "            saved_images = list(reversed(st.session_state.saved_images))\n",
        "\n",
        "            for i in range(0, len(saved_images), num_cols):\n",
        "                cols = st.columns(num_cols)\n",
        "                for j in range(num_cols):\n",
        "                    if i + j < len(saved_images):\n",
        "                        img_info = saved_images[i + j]\n",
        "                        with cols[j]:\n",
        "                            st.image(\n",
        "                                img_info[\"image\"],\n",
        "                                caption=img_info[\"prompt\"][:40] + \"...\" if len(img_info[\"prompt\"]) > 40 else img_info[\"prompt\"],\n",
        "                                use_column_width=True,\n",
        "                                output_format=\"PNG\",\n",
        "                                clamp=True\n",
        "                            )\n",
        "\n",
        "                            # Show when the image was created\n",
        "                            st.caption(f\"Created: {img_info['timestamp']}\")\n",
        "\n",
        "                            # Download button for each gallery image\n",
        "                            download_link = get_image_download_link(\n",
        "                                img_info[\"image\"],\n",
        "                                f\"sd_image_{img_info['timestamp']}.png\",\n",
        "                                \"📥 Download\"\n",
        "                            )\n",
        "                            st.markdown(download_link, unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rer92wLkRQRo",
        "outputId": "bbfdae64-d1c7-43ca-b7ab-0fba00bfb951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "nohup: failed to run command 'streamlit': No such file or directory\n",
            "/bin/bash: line 1: ./cloudflared: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Start the Streamlit app\n",
        "!nohup streamlit run app.py --server.port 8501 &\n",
        "\n",
        "# Start Cloudflare tunnel\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRIF18AQR6uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4037b7a-0bbb-4838-a105-b1b704571dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install flask tensorflow keras numpy pandas scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "import json\n",
        "import os\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "\n",
        "# Define models\n",
        "class FinancialAlert:\n",
        "    def __init__(self, user_id, title, amount, due_date, frequency='MONTHLY'):\n",
        "        self.user_id = user_id\n",
        "        self.title = title\n",
        "        self.amount = amount\n",
        "        self.due_date = datetime.strptime(due_date, '%Y-%m-%d').date()\n",
        "        self.frequency = frequency\n",
        "        self.is_active = True\n",
        "\n",
        "class Expense:\n",
        "    def __init__(self, user_id, amount, category, date):\n",
        "        self.user_id = user_id\n",
        "        self.amount = amount\n",
        "        self.category = category\n",
        "        self.date = datetime.strptime(date, '%Y-%m-%d').date()\n",
        "\n",
        "# Mock database\n",
        "class Database:\n",
        "    def __init__(self):\n",
        "        self.alerts = []\n",
        "        self.expenses = []\n",
        "        self.users = {}\n",
        "\n",
        "    def add_alert(self, alert):\n",
        "        self.alerts.append(alert)\n",
        "        return alert\n",
        "\n",
        "    def get_user_alerts(self, user_id):\n",
        "        return [alert for alert in self.alerts if alert.user_id == user_id and alert.is_active]\n",
        "\n",
        "    def add_expense(self, expense):\n",
        "        self.expenses.append(expense)\n",
        "        return expense\n",
        "\n",
        "    def get_user_expenses(self, user_id, category=None):\n",
        "        if category:\n",
        "            return [exp for exp in self.expenses if exp.user_id == user_id and exp.category == category]\n",
        "        return [exp for exp in self.expenses if exp.user_id == user_id]\n",
        "\n",
        "# Alert Service\n",
        "class AlertService:\n",
        "    def __init__(self, db):\n",
        "        self.db = db\n",
        "\n",
        "    def predict_payment_date(self, user_id, alert_type):\n",
        "        expenses = self.db.get_user_expenses(user_id, alert_type)\n",
        "        if len(expenses) < 3:\n",
        "            return None\n",
        "\n",
        "        dates = np.array([(exp.date - expenses[0].date).days for exp in expenses]).reshape(-1, 1)\n",
        "        amounts = np.array([exp.amount for exp in expenses])\n",
        "\n",
        "        model = LinearRegression()\n",
        "        model.fit(dates, amounts)\n",
        "\n",
        "        next_date = dates[-1] + 30\n",
        "        predicted_amount = model.predict([[next_date[0]]])[0]\n",
        "\n",
        "        return {\n",
        "            'predicted_date': expenses[0].date + timedelta(days=int(next_date[0])),\n",
        "            'predicted_amount': round(predicted_amount, 2)\n",
        "        }\n",
        "\n",
        "    def get_due_alerts(self, user_id):\n",
        "        today = datetime.now().date()\n",
        "        return [alert for alert in self.db.get_user_alerts(user_id)\n",
        "                if 0 <= (alert.due_date - today).days <= 7]\n",
        "\n",
        "# Chatbot\n",
        "class FinancialChatbot:\n",
        "    def __init__(self):\n",
        "        self.db = Database()\n",
        "        self.alert_service = AlertService(self.db)\n",
        "\n",
        "    def handle_alert_command(self, statement, user_id):\n",
        "        if statement.lower() == \"show my alerts\":\n",
        "            alerts = self.alert_service.get_due_alerts(user_id)\n",
        "            if not alerts:\n",
        "                return \"You have no upcoming payments due.\"\n",
        "\n",
        "            response = \"Here are your upcoming payments:\\n\"\n",
        "            for alert in alerts:\n",
        "                days_until_due = (alert.due_date - datetime.now().date()).days\n",
        "                response += f\"- {alert.title}: ${alert.amount} due in {days_until_due} days ({alert.due_date})\\n\"\n",
        "            return response\n",
        "\n",
        "        if statement.lower().startswith(\"create alert:\"):\n",
        "            try:\n",
        "                _, details = statement.split(\":\", 1)\n",
        "                title, amount, due_date, frequency = [x.strip() for x in details.split(\"|\")]\n",
        "\n",
        "                alert = FinancialAlert(\n",
        "                    user_id=user_id,\n",
        "                    title=title,\n",
        "                    amount=float(amount),\n",
        "                    due_date=due_date,\n",
        "                    frequency=frequency.upper()\n",
        "                )\n",
        "                self.db.add_alert(alert)\n",
        "                return f\"Alert created successfully for {title}\"\n",
        "            except Exception as e:\n",
        "                return \"Invalid alert format. Please use: create alert: title | amount | due_date | frequency\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def process_message(self, message, user_id):\n",
        "        alert_response = self.handle_alert_command(message, user_id)\n",
        "        if alert_response:\n",
        "            return alert_response\n",
        "        return \"I'm a financial chatbot. I can help you with alerts and expenses.\"\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "chatbot = FinancialChatbot()\n",
        "\n",
        "# HTML template\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Financial Chatbot</title>\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            margin: 0;\n",
        "            padding: 20px;\n",
        "            background-color: #f5f5f5;\n",
        "        }\n",
        "        .chat-container {\n",
        "            max-width: 800px;\n",
        "            margin: 0 auto;\n",
        "            background: white;\n",
        "            border-radius: 10px;\n",
        "            box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
        "            padding: 20px;\n",
        "        }\n",
        "        .chat-header {\n",
        "            text-align: center;\n",
        "            padding: 10px;\n",
        "            background: #2c3e50;\n",
        "            color: white;\n",
        "            border-radius: 5px;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .chat-messages {\n",
        "            height: 400px;\n",
        "            overflow-y: auto;\n",
        "            padding: 10px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .message {\n",
        "            margin: 10px 0;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "        }\n",
        "        .user-message {\n",
        "            background: #e3f2fd;\n",
        "            margin-left: 20%;\n",
        "        }\n",
        "        .bot-message {\n",
        "            background: #f1f1f1;\n",
        "            margin-right: 20%;\n",
        "        }\n",
        "        .input-container {\n",
        "            display: flex;\n",
        "            gap: 10px;\n",
        "        }\n",
        "        input[type=\"text\"] {\n",
        "            flex: 1;\n",
        "            padding: 10px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "        }\n",
        "        button {\n",
        "            padding: 10px 20px;\n",
        "            background: #2c3e50;\n",
        "            color: white;\n",
        "            border: none;\n",
        "            border-radius: 5px;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "        button:hover {\n",
        "            background: #34495e;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"chat-container\">\n",
        "        <div class=\"chat-header\">\n",
        "            <h2>Financial Chatbot</h2>\n",
        "        </div>\n",
        "        <div class=\"chat-messages\" id=\"chat-messages\">\n",
        "            <div class=\"message bot-message\">\n",
        "                Hello! I'm your financial assistant. How can I help you today?\n",
        "            </div>\n",
        "        </div>\n",
        "        <div class=\"input-container\">\n",
        "            <input type=\"text\" id=\"user-input\" placeholder=\"Type your message here...\">\n",
        "            <button onclick=\"sendMessage()\">Send</button>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        function sendMessage() {\n",
        "            const input = document.getElementById('user-input');\n",
        "            const message = input.value.trim();\n",
        "            if (message) {\n",
        "                // Add user message to chat\n",
        "                addMessage(message, 'user');\n",
        "                input.value = '';\n",
        "\n",
        "                // Send message to backend\n",
        "                fetch('/chat', {\n",
        "                    method: 'POST',\n",
        "                    headers: {\n",
        "                        'Content-Type': 'application/json',\n",
        "                    },\n",
        "                    body: JSON.stringify({ message: message, user_id: 1 })\n",
        "                })\n",
        "                .then(response => response.json())\n",
        "                .then(data => {\n",
        "                    addMessage(data.response, 'bot');\n",
        "                });\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function addMessage(text, sender) {\n",
        "            const chatMessages = document.getElementById('chat-messages');\n",
        "            const messageDiv = document.createElement('div');\n",
        "            messageDiv.className = `message ${sender}-message`;\n",
        "            messageDiv.textContent = text;\n",
        "            chatMessages.appendChild(messageDiv);\n",
        "            chatMessages.scrollTop = chatMessages.scrollHeight;\n",
        "        }\n",
        "\n",
        "        // Handle Enter key\n",
        "        document.getElementById('user-input').addEventListener('keypress', function(e) {\n",
        "            if (e.key === 'Enter') {\n",
        "                sendMessage();\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    data = request.json\n",
        "    message = data['message']\n",
        "    user_id = data['user_id']\n",
        "    response = chatbot.process_message(message, user_id)\n",
        "    return jsonify({'response': response})\n",
        "\n",
        "# Run the Flask app\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var='''Hi'''\n",
        "print(var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZuH_CioJVyt",
        "outputId": "1515a94b-6088-416a-a490-edd13c56a31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Hi'''\n",
        "print(\"Heloo\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdB9ykbye3vx",
        "outputId": "90413da1-e417-4130-bc37-7f31a62db064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heloo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=10\n",
        "b=10\n",
        "print(id(a))\n",
        "print(id(b))\n",
        "a=20\n",
        "print(a)\n",
        "print(id(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1nr8OKme6bS",
        "outputId": "f2b09554-1a16-49b7-cd68-9d71edaf9ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10751144\n",
            "10751144\n",
            "20\n",
            "10751464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=-6\n",
        "b=-6\n",
        "print(id(a))\n",
        "print(id(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx4y2_61gIVs",
        "outputId": "8f93f121-a842-40de-891b-6d1f006c0844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137991516628048\n",
            "137991516629680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVKoCadcgo_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}